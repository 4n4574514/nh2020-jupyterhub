nfs:
  enabled: true
  # Use the output from the command below to set serverIP and serverName.
  # Inspect fileShares.0.name for the serverName and networks.0.ipAddresses.0
  # for the serverIP.
  #
  #   gcloud beta filestore instances describe nh-2020 --zone=us-east1-b
  #
  serverIP: 10.60.0.18
  serverName: nh
  
jupyterhub:
  debug:
    enabled: true

  ## ingress: should be enabled if we transition to use nginx-ingress +
  ## cert-manager.
  ##
  # ingress:
  #   enabled: true
  #   annotations:
  #     kubernetes.io/tls-acme: "true"
  #     kubernetes.io/ingress.class: nginx
  #   hosts:
  #     - hub.neurohackademy.org
  #   tls:
  #     - secretName: jupyterhub-tls
  #       hosts:
  #         - hub.neurohackademy.org

  prePuller:
    hook:
      enabled: false
    continuous:
      enabled: true

  scheduling:
    userScheduler:
      enabled: true
      replicas: 2
    podPriority:
      enabled: true
    userPlaceholder:
      enabled: true
      replicas: 1
    corePods:
      nodeAffinity:
        matchNodePurpose: require
    userPods:
      nodeAffinity:
        matchNodePurpose: require

  singleuser:
    ## cmd: set this to start-singleuser.sh if we use a docker-stacks image,
    ## repo2docker does not come with that but the jupyter-singleuser command is
    ## part of JupyterHub though.
    ##
    # cmd: start-singleuser.sh
    defaultUrl: /lab
    startTimeout: 900
    ## cpu/memory requests:
    ## We want to fit as many users on a m1-ultramem-40 node but still ensure
    ## they get up to 24 GB of ram.
    cpu:
      guarantee: 0.36 # guarantee as much as possible for 110 pods (max per
                      # node) to fit on a 40 CPU machine
      limit: 16       # allow for a lot more CPU to be used
    memory:
      guarantee: 24G
    # initContainers:
    # We may want this to ensure whatever dataset is mounted through NFS is
    # readable for jovyan.
    #
    initContainers:
      - name: chown-nfs-mount-to-jovyan
        image: busybox
        command:
          - "sh"
          - "-c"
          - "id && chown 1000:1000 /nh/data && ls -lhd /nh/data"
        securityContext:
          runAsUser: 0
        volumeMounts:
          - name: nh
            mountPath: /nh/data
            subPath: data

  hub:
    extraVolumes:
      - name: hub-etc-jupyterhub-acl
        secret:
          secretName: hub-etc-jupyterhub-acl
      - name: hub-etc-jupyterhub-templates
        configMap:
          name: hub-etc-jupyterhub-templates
      - name: hub-usr-local-share-jupyterhub-static-external
        configMap:
          name: hub-usr-local-share-jupyterhub-static-external
    extraVolumeMounts:
      - mountPath: /etc/jupyterhub/acl.yaml
        name: hub-etc-jupyterhub-acl
        subPath: acl.yaml
      - mountPath: /etc/jupyterhub/templates
        name: hub-etc-jupyterhub-templates
      - mountPath: /usr/local/share/jupyterhub/static/external
        name: hub-usr-local-share-jupyterhub-static-external
    extraConfig:
      # announcements: |
      #   c.JupyterHub.template_vars.update({
      #       'announcement': 'Any message we want to pass to instructors?',
      #   })
      auth: |
        # Don't wait for users to press the orange button to login.
        c.Authenticator.auto_login = True
      templates: |
        # Help JupyterHub find the templates we may mount
        c.JupyterHub.template_paths.insert(0, "/etc/jupyterhub/templates")
      metrics: |
        # With this setting set to False, the /hub/metrics endpoint will be
        # publically accessible just like at hub.mybinder.org/hub/metrics is.
        c.JupyterHub.authenticate_prometheus = False
      spawn: |
        # Override the working directory of /src/repo which repo2docker have set
        # to /home/jovyan instead, where we mount of files.
        c.KubeSpawner.extra_container_config = {
            "workingDir": "/home/jovyan",
        }



        # Configure what spawn options users should see
        # ---------------------------------------------
        #
        # NOTE: setting c.KubeSpawner.profile_list directly is easier, but then
        #       we don't have the option to adjust it based on the individual
        #       user at a later point in time if we want.
        #
        # NOTE: c.KubeSpawner.options_form, defined in the Spawner base class,
        #       can be set to a fixed value, but it can also be a callable
        #       function that returns a value. If this returned value is falsy,
        #       no form will be rendered. In this case, we setup a callable
        #       function that relies on KubeSpawner's internal logic to create
        #       an options_form from the profile_list configuration.
        #
        #       ref: https://github.com/jupyterhub/jupyterhub/pull/2415
        #       ref: https://github.com/jupyterhub/jupyterhub/issues/2390
        #
        async def dynamic_options_form(self):
            self.profile_list = [
                {
                    'default': True,
                    'display_name': '24GB RAM and up to 16 CPU',
                    'kubespawner_override': { 'mem_guarantee': '24G' },
                },
                {
                    'display_name': '12GB RAM and up to 16 CPU',
                    'kubespawner_override': { 'mem_guarantee': '12G' },
                },
                {
                    'display_name': '6GB RAM and up to 16 CPU',
                    'kubespawner_override': { 'mem_guarantee':' 6G' },
                },
            ]

            # NOTE: We let KubeSpawner inspect profile_list and decide what to
            #       return, it will return a falsy blank string if there is no
            #       profile_list, which makes no options form be presented.
            #
            # ref: https://github.com/jupyterhub/kubespawner/blob/37a80abb0a6c826e5c118a068fa1cf2725738038/kubespawner/spawner.py#L1885-L1935
            #
            return self._options_form_default()

        c.KubeSpawner.options_form = dynamic_options_form



        # Configure storage details etc. based on the user
        # ------------------------------------------------
        #
        async def pre_spawn_hook(spawner):
            username = spawner.user.name

            # Configure the pod's labels
            spawner.extra_labels.update({
                "hub.neurohackademy.org/is-admin": str(is_admin(username)).lower(),
                "hub.neurohackademy.org/is-instructor": str(is_instructor(username)).lower(),
                "hub.neurohackademy.org/is-participant": str(is_participant(username)).lower(),
            })

            # Configure the pod's storage
            nh_volume = {
                "name": "nh",
                "persistentVolumeClaim": {
                    "claimName": "nfs-pvc",
                },
            }
            nh_volume_mount = {
                "name": "nh",
                "mountPath": "/nh/data",  # where it is made available in container
                "subPath": "data",        # what in the PVC to mount (must be a relative path)
                "readOnly": not (is_admin(username) or is_instructor(username)),
            }
            spawner.volumes.extend([nh_volume])
            spawner.volume_mounts.extend([nh_volume_mount])

            # Configure the pod's container's environment variables
            spawner.environment.update({})

        c.KubeSpawner.pre_spawn_hook = pre_spawn_hook

  proxy:
    https:
      enabled: true
      hosts: [hub.neurohackademy.org]
    service:
      type: LoadBalancer
      loadBalancerIP: 34.75.11.207

  cull:
    enabled: true
    timeout: 7200 # 2 hours in seconds
    maxAge: 0 # Allow pods to run forever

# Reference on the Grafana Helm chart's configuration options:
# https://github.com/helm/charts/blob/master/stable/grafana/values.yaml
grafana:
  # Reference on Grafana's configuration options:
  # http://docs.grafana.org/installation/configuration
  grafana.ini:
    log:
      level: debug
    server:
      domain: hub.neurohackademy.org
      # NOTE: Don't use %(protocol)s in root_url, but hardcode https. If not, it
      #       will when redirecting the user to external authentication set with
      #       a redirect back query parameter to use http instead of https,
      #       which will be wrong. This is because the TLS termination is done
      #       without Grafanas knowledge by the ingress controller. If we would
      #       specify protocol to be https, then it would want to do the TLS
      #       termination itself so that also would fail.
      root_url: 'https://%(domain)s/services/grafana'
      serve_from_sub_path: true
      enforce_domain: true
      enable_gzip: true
      router_logging: true
