# JupyterHub Access Control List, not only does it decide what users will be
# allowed access, but it will also be used to influence logic in a
# pre_spawn_hook that can attach storage etc depending on user type.
acl.yaml:
  admins:
    - arokem
    - consideRatio
  instructors:
    - consideRatio
  participants:
    - consideRatio-demo

nfs:
  enabled: true
  # Use the output from the command below to set serverIP and serverName.
  # Inspect fileShares.0.name for the serverName and networks.0.ipAddresses.0
  # for the serverIP.
  #
  #   gcloud beta filestore instances describe nh-2020 --zone=us-east1-b
  #
  serverIP: 10.60.0.18
  serverName: nh
  
jupyterhub:
  debug:
    enabled: true

  ## ingress: should be enabled if we transition to use nginx-ingress +
  ## cert-manager.
  ##
  # ingress:
  #   enabled: true
  #   annotations:
  #     kubernetes.io/tls-acme: "true"
  #     kubernetes.io/ingress.class: nginx
  #   hosts:
  #     - hub.neurohackademy.org
  #   tls:
  #     - secretName: jupyterhub-tls
  #       hosts:
  #         - hub.neurohackademy.org

  prePuller:
    hook:
      enabled: false
    continuous:
      enabled: true

  scheduling:
    userScheduler:
      enabled: true
      replicas: 2
    podPriority:
      enabled: true
    userPlaceholder:
      enabled: true
      replicas: 0
    corePods:
      nodeAffinity:
        matchNodePurpose: require
    userPods:
      nodeAffinity:
        matchNodePurpose: require

  singleuser:
    ## cmd: set this to start-singleuser.sh if we use a docker-stacks image,
    ## repo2docker does not come with that but the jupyter-singleuser command is
    ## part of JupyterHub though.
    ##
    # cmd: start-singleuser.sh
    defaultUrl: /lab
    startTimeout: 900
    ## cpu/memory requests:
    ## We want to fit as many users on a m1-ultramem-40 node but still ensure
    ## they get up to 24 GB of ram. At this point during setup, we want to also
    ## allow a user to start on the n1-standard-4 node to save money.
    cpu:
      guarantee: 0.475
      limit: 40
    memory:
      guarantee: 0.5G
      limit: 24G
    # initContainers:
    # We may want this to ensure whatever dataset is mounted through NFS is
    # readable for jovyan.
    #
    initContainers:
      - name: chown-nfs-mount-to-jovyan
        image: busybox
        command:
          - "sh"
          - "-c"
          - "id && chown 1000:1000 /nh/data && ls -lhd /nh/data"
        securityContext:
          runAsUser: 0
        volumeMounts:
          - name: nh
            mountPath: /nh/data
            subPath: data

  hub:
    extraVolumes:
      - name: hub-etc-jupyterhub-acl
        secret:
          secretName: hub-etc-jupyterhub-acl
      - name: hub-etc-jupyterhub-templates
        configMap:
          name: hub-etc-jupyterhub-templates
      - name: hub-usr-local-share-jupyterhub-static-external
        configMap:
          name: hub-usr-local-share-jupyterhub-static-external
    extraVolumeMounts:
      - mountPath: /etc/jupyterhub/acl.yaml
        name: hub-etc-jupyterhub-acl
        subPath: acl.yaml
      - mountPath: /etc/jupyterhub/templates
        name: hub-etc-jupyterhub-templates
      - mountPath: /usr/local/share/jupyterhub/static/external
        name: hub-usr-local-share-jupyterhub-static-external
    extraConfig:
      # announcements: |
      #   c.JupyterHub.template_vars.update({
      #       'announcement': 'Any message we want to pass to instructors?',
      #   })
      auth: |
        # Don't wait for users to press the orange button to login.
        c.Authenticator.auto_login = True
      templates: |
        # Help JupyterHub find the templates we may mount
        c.JupyterHub.template_paths.insert(0, "/etc/jupyterhub/templates")
      metrics: |
        # With this setting set to False, the /hub/metrics endpoint will be
        # publically accessible just like at hub.mybinder.org/hub/metrics is.
        c.JupyterHub.authenticate_prometheus = False
      spawn: |
        # Override the working directory of /src/repo which repo2docker have set
        # to /home/jovyan instead, where we mount of files.
        c.KubeSpawner.extra_container_config = {
            "workingDir": "/home/jovyan",
        }

        # Invoke logic based on username and its access before we spawn the pod
        # with the users individual spawner instance.
        async def pre_spawn_hook(spawner):
            username = spawner.user.name

            # Configure the pod's labels
            spawner.extra_labels.update({
                "hub.neurohackademy.org/is-admin": str(is_admin(username)).lower(),
                "hub.neurohackademy.org/is-instructor": str(is_instructor(username)).lower(),
                "hub.neurohackademy.org/is-participant": str(is_participant(username)).lower(),
            })

            # Configure the pod's storage
            nh_volume = {
                "name": "nh",
                "persistentVolumeClaim": {
                    "claimName": "nfs-pvc",
                },
            }
            nh_volume_mount = {
                "name": "nh",
                "mountPath": "/nh/data",  # where it is made available in container
                "subPath": "data",        # what in the PVC to mount (must be a relative path)
                "readOnly": not (is_admin(username) or is_instructor(username)),
            }
            spawner.volumes.extend([nh_volume])
            spawner.volume_mounts.extend([nh_volume_mount])

            # Configure the pod's container's environment variables
            spawner.environment.update({})

        c.KubeSpawner.pre_spawn_hook = pre_spawn_hook

  proxy:
    https:
      enabled: true
      hosts: [hub.neurohackademy.org]
    service:
      type: LoadBalancer
      loadBalancerIP: 34.75.11.207

  cull:
    enabled: true
    timeout: 7200 # 2 hours in seconds
    maxAge: 0 # Allow pods to run forever

# Reference on the Grafana Helm chart's configuration options:
# https://github.com/helm/charts/blob/master/stable/grafana/values.yaml
grafana:
  # Reference on Grafana's configuration options:
  # http://docs.grafana.org/installation/configuration
  grafana.ini:
    log:
      level: debug
    server:
      domain: hub.neurohackademy.org
      # NOTE: Don't use %(protocol)s in root_url, but hardcode https. If not, it
      #       will when redirecting the user to external authentication set with
      #       a redirect back query parameter to use http instead of https,
      #       which will be wrong. This is because the TLS termination is done
      #       without Grafanas knowledge by the ingress controller. If we would
      #       specify protocol to be https, then it would want to do the TLS
      #       termination itself so that also would fail.
      root_url: 'https://%(domain)s/services/grafana'
      serve_from_sub_path: true
      enforce_domain: true
      enable_gzip: true
      router_logging: true
